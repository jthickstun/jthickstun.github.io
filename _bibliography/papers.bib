---
---

@inproceedings{donahue2022melody,
  abstract  = {Despite the central role that melody plays in music perception, it remains an open challenge in music information retrieval to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in melody transcription is building methods which can handle broad audio containing any number of instrument ensembles and musical styles - existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data - we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build Sheet Sage, a system capable of transcribing human-readable lead sheets directly from music audio.},
  title     = {Melody transcription via generative pre-training},
  author    = {Chris Donahue and John Thickstun and Percy Liang},
  booktitle = {International Society for Music Information Retrieval},
  year      = {2022},
  abbr      = {ISMIR},
  arxiv     = {2212.01884},
  code      = {https://github.com/chrisdonahue/sheetsage},
  website   = {https://chrisdonahue.com/sheetsage},
  selected  = {true}
}

@article{benster2022reconstruction,
  abstract  = {All visual information in mammals is encoded in the aggregate pattern of retinal ganglion cell (RGC) firing. How this information is decoded to yield percepts remains incompletely understood. We have trained convolutional neural networks with multielectrode array-recorded murine RGC responses to projected images. The trained model accurately reconstructed novel facial images solely from RGC firing data. In this model, subpopulations of cells with faster firing rates are largely sufficient for accurate reconstruction, and ON- and OFF-cells contribute complementary and overlapping information to image reconstruction. Information content for reconstruction correlates with overall firing rate, and locality of information contributing to reconstruction varies substantially across the image and retina. This model demonstrates that artificial neural networks are capable of learning multicellular sensory neural encoding, and provides a viable model for understanding visual information encoding.},
  title     = {Reconstruction of visual images from mouse retinal ganglion cell spiking activity using convolutional neural networks},
  author    = {Tyler Benster and Darwin Babino and John Thickstun and Matthew Hunt and Xiyang Liu and Zaid Harchaoui and Sewoong Oh and Russell N Van Gelder},
  year      = {2022},
  abbr      = {bioRxiv},
  pdf       = {https://www.biorxiv.org/content/10.1101/2022.06.10.482188v1.abstract},
  code      = {https://github.com/tbenst/2021-retina-reconstructions}
}

@inproceedings{li2022diffusion,
  abstract  = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  title     = {Diffusion-LM improves controllable text generation},
  author    = {Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori B. Hashimoto},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022},
  abbr      = {Neurips},
  award     = {Oral Presentation},
  arxiv     = {2205.14217},
  code      = {https://github.com/XiangLi1999/Diffusion-LM},
  slides    = {diffusion-lm.pdf},
  selected  = {true}
}

@phdthesis{thickstun2021leveraging,
  title     = {Leveraging generative models for music and signal processing},
  author    = {John Thickstun},
  year      = {2021},
  school    = {University of Washington},
  abbr      = {Dissertation},
  pdf       = {thickstun_thesis.pdf},
  selected  = {true}
}

@inproceedings{pillutla2021mauve,
  abstract  = {As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.},
  title     = {MAUVE: measuring the gap between neural text and human text using divergence frontiers},
  author    = {Krishna Pillutla and Swabha Swayamdipta and Rowan Zellers and John Thickstun and Sean Welleck and Yejin Choi and Zaid Harchaoui},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021},
  abbr      = {Neurips},
  award     = {Outstanding Paper},
  arxiv     = {2102.01454},
  code      = {https://github.com/krishnap25/mauve-experiments},
  selected  = {true}
}

@inproceedings{jayaram2021parallel,
  abstract  = {This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.},
  title     = {Parallel and flexible sampling from autoregressive models via {L}angevin dynamics},
  author    = {Vivek Jayaram and John Thickstun},
  booktitle = {International Conference on Machine Learning},
  volume    = {139},
  pages     = {4807--4818},
  year      = {2021},
  publisher = {PMLR},
  abbr      = {ICML},
  arxiv     = {2105.08164},
  code      = {https://github.com/vivjay30/pnf-sampling/},
  poster    = {thickstun_langevin_poster.pdf},
  website   = {https://grail.cs.washington.edu/projects/pnf-sampling/},
  selected  = {true}
}

@inproceedings{ainsworth2021faster,
  abstract  = {We study the estimation of policy gradients for continuous-time systems with known dynamics. By reframing policy learning in continuous-time, we show that it is possible construct a more efficient and accurate gradient estimator. The standard back-propagation through time estimator (BPTT) computes exact gradients for a crude discretization of the continuous-time system. In contrast, we approximate continuous-time gradients in the original system. With the explicit goal of estimating continuous-time gradients, we are able to discretize adaptively and construct a more efficient policy gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We show that replacing BPTT policy gradients with more efficient CTPG estimates results in faster and more robust learning in a variety of control tasks and simulators.},
  title     = {Faster policy learning with continuous-time gradients},
  author    = {Samuel K. Ainsworth and Kendall Lowrey and John Thickstun and Zaid Harchaoui and Siddhartha S. Srinivasa},
  booktitle = {Learning for Dynamics and Control},
  volume    = {144},
  pages     = {1054--1067},
  year      = {2021},
  publisher = {PMLR},
  abbr      = {L4DC},
  arxiv     = {2012.06684},
  code      = {https://github.com/samuela/ctpg},
}

@article{thickstun2020rethinking,
  abstract  = {This paper offers a precise, formal definition of an audio-to-score alignment. While the concept of an alignment is intuitively grasped, this precision affords us new insight into the evaluation of audio-to-score alignment algorithms. Motivated by these insights, we introduce new evaluation metrics for audio-to-score alignment. Using an alignment evaluation dataset derived from pairs of KernScores and MAESTRO performances, we study the behavior of our new metrics and the standard metrics on several classical alignment algorithms.},
  title     = {Rethinking evaluation methodology for audio-to-score alignment},
  author    = {John Thickstun and Jennifer Brennan and Harsh Verma},
  year      = {2020},
  abbr      = {ArXiv},
  arxiv     = {2009.14374},
  code      = {https://github.com/jthickstun/alignment-eval}
}

@inproceedings{paranjape2020information,
  abstract  = {Decisions of complex language understanding models can be rationalized by limiting their inputs to a relevant subsequence of the original text. A rationale should be as concise as possible without significantly degrading task performance, but this balance can be difficult to achieve in practice. In this paper, we show that it is possible to better manage this trade-off by optimizing a bound on the Information Bottleneck (IB) objective. Our fully unsupervised approach jointly learns an explainer that predicts sparse binary masks over sentences, and an end-task predictor that considers only the extracted rationale. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on ERASER benchmark tasks demonstrate significant gains over norm-minimization techniques for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25\% of training examples) closes the gap with a model that uses the full input.},
  title     = {An information bottleneck approach for controlling conciseness in rationale extraction},
  author    = {Bhargavi Paranjape and Mandar Joshi and John Thickstun and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  pages     = {1938--1952},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  abbr      = {EMNLP},
  arxiv     = {2005.00652},
  code      = {https://github.com/bhargaviparanjape/explainable_qa},
  slides    = {https://bhargaviparanjape.github.io/documents/slides/EMNLP2020_SPARSE_PRIORS.pdf},
}

@inproceedings{jayaram2020source,
  abstract  = {Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.},
  title     = {Source separation with deep generative priors},
  author    = {Vivek Jayaram and John Thickstun},
  booktitle = {International Conference on Machine Learning},
  volume    = {119},
  pages     = {4724--4735},
  year      = {2020},
  publisher = {PMLR},
  abbr      = {ICML},
  arxiv     = {2002.07942},
  code      = {https://github.com/jthickstun/basis-separation},
  poster    = {thickstun_jayaram_sourcesep_poster.pdf},
  selected  = {true}
}


@inproceedings{verma2019convolutional,
  abstract  = {This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works.},
  author    = {Harsh Verma and John Thickstun},
  title     = {Convolutional composer classification},
  booktitle = {International Society for Music Information Retrieval},
  pages     = {549--556},
  year      = {2019},
  abbr      = {ISMIR},
  arxiv     = {1911.11737},
  poster    = {verma_composers_poster.pdf},
  code      = {https://github.com/harshshredding/Convolutional-Composer-Classification}
}

@inproceedings{thickstun2018coupled,
  abstract  = {This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset.},
  title     = {Coupled recurrent models for polyphonic music composition},
  author    = {John Thickstun and Zaid Harchaoui and Dean P Foster and Sham M Kakade},
  booktitle = {International Society for Music Information Retrieval},
  pages     = {311--318},
  year      = {2019},
  abbr      = {ISMIR},
  arxiv     = {1811.08045},
  code      = {https://github.com/jthickstun/ismir2019coupled},
  poster    = {thickstun_coupled_poster.pdf}
}

@inproceedings{thickstun2018invariances,
  abstract  = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
  title     = {Invariances and data augmentation for supervised music transcription},
  author    = {John Thickstun and Zaid Harchaoui and Dean P Foster and Sham M Kakade},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing},
  publisher = {IEEE},
  pages     = {2241--2245},
  year      = {2018},
  abbr      = {ICASSP},
  award     = {Oral Presentation},
  arxiv     = {1711.04845},
  code      = {https://github.com/jthickstun/thickstun2018invariances},
  selected  = {true}
}

@article{thickstun2017mirex,
  abstract  = {This document describes the THK1 submission in the 2017 MIREX Multi-F0 competition. The model is a convolutional neural network, trained using the MusicNet labels. Its input is a bank of logarithmically-spaced frequency filters. These filters exhibit translation invariance along the log-frequency axis, which is captured in this model by one-dimensional convolutions along the frequency axis. The model fully connects across the time axis to capture temporal dependencies. The training data is augmented by pitch-shifting the original data by up to 5 semitones and applying small (up to 1/10 semitone) continuous pitch jitter to the input.},
  title     = {Frequency domain convolutions for multiple F0 estimation},
  author    = {John Thickstun and Zaid Harchaoui and Dean P Foster and Sham M Kakade},
  year      = {2017},
  abbr      = {MIREX},
  pdf       = {https://www.music-ir.org/mirex/abstracts/2017/THK1.pdf}
}

@inproceedings{thickstun2017learning,
  abstract={This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
  author    = {John Thickstun and Zaid Harchaoui and Sham M Kakade},
  title     = {Learning features of music from scratch},
  booktitle = {International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2017},
  abbr      = {ICLR},
  arxiv     = {1611.09827},
  code      = {https://github.com/jthickstun/thickstun2017learning},
  poster    = {thickstun2017learning_poster.pdf},
  website   = {https://zenodo.org/record/5120004#.YaaHUlOIZpR},
  selected  = {true}
}

